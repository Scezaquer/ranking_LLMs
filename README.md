# Rankings of LLMs as a function of the rating algorithm used

| Model name | BayesElo | Elo | Sequential Elo | Glicko | Glicko 2 | TrueSkill | mElo2 | mElo4 | mElo10 | mElo20 | Winrate | Drawrate | Loserate | WDL | Wins | Draws | Losses | Total played |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| gpt-4 | 212.7 | 8470.0 | 235.4 | 280.1 | 185.6 | 29.5 | 0.04797  | 0.06507  | 0.03813  | 0.04818 | 64.7 | 24.5 | 10.8 | 4235 | 5080 | 1924 | 845 | 7849
| claude-v1 | 171.6 | 5550.0 | 159.5 | 220.7 | 145.3 | 27.9 | 0.03543  | 0.04362  | 0.02918  | 0.02834 | 57.6 | 27.3 | 15.1 | 2775 | 3759 | 1783 | 984 | 6526
| claude-instant-v1 | 148.4 | 3452.0 | 177.1 | 185.6 | 120.6 | 28.9 | 0.02475  | 0.0331  | 0.02002  | 0.02158 | 52.8 | 30.1 | 17.1 | 1726 | 2550 | 1453 | 824 | 4827
| gpt-3.5-turbo | 128.7 | 5048.0 | 120.9 | 169.7 | 111.3 | 26.9 | 0.02637  | 0.04147  | 0.02152  | 0.02071 | 52.4 | 27.8 | 19.7 | 2524 | 4048 | 2148 | 1524 | 7720
| vicuna-13b | 50.0 | 12014.0 | 69.3 | 141.1 | 93.6 | 27.1 | 0.0506  | 0.04165  | 0.02344  | 0.02702 | 48.9 | 29.4 | 21.7 | 6007 | 10809 | 6488 | 4802 | 22099
| vicuna-33b | 103.3 | 1132.0 | 114.2 | 109.2 | 65.5 | 27.5 | 0.01692  | 0.01215  | 0.01762  | 0.01223 | 42.8 | 35.5 | 21.7 | 566 | 1151 | 954 | 585 | 2690
| mpt-30b-chat | 59.6 | 332.0 | 64.5 | 34.6 | 16.9 | 26.7 | 0.00849  | 0.00165  | 0.00491  | 0.0011 | 34.6 | 37.5 | 27.9 | 166 | 861 | 933 | 695 | 2489
| palm-2 | 19.7 | -268.0 | 41.3 | -13.2 | -7.3 | 26.2 | -0.00645  | -0.00121  | -0.00669  | -0.00239 | 32.1 | 33.2 | 34.7 | -134 | 1694 | 1751 | 1828 | 5273
| wizardlm-13b | 48.6 | 484.0 | 38.5 | 28.0 | 15.4 | 25.7 | 0.00653  | 0.00765  | -0.00077  | 0.00198 | 31.1 | 43.2 | 25.7 | 242 | 1394 | 1938 | 1152 | 4484
| koala-13b | -7.0 | -1972.0 | -15.2 | -40.3 | -25.4 | 25.1 | -0.0048  | -0.00608  | -0.00533  | 0.00111 | 29.7 | 32.9 | 37.4 | -986 | 3773 | 4179 | 4759 | 12711
| guanaco-33b | 42.8 | 42.0 | 42.1 | 2.5 | 1.3 | 26.1 | 0.00904  | -0.0053  | 0.00314  | 0.00186 | 29.5 | 41.5 | 29.0 | 21 | 1286 | 1806 | 1265 | 4357
| vicuna-7b | 9.3 | -948.0 | -9.4 | -43.2 | -25.6 | 25.0 | -0.01066  | -0.01042  | -0.00613  | -0.0034 | 28.1 | 35.4 | 36.5 | -474 | 1603 | 2016 | 2077 | 5696
| chatglm-6b | -62.1 | -2712.0 | -64.6 | -95.8 | -61.3 | 23.9 | -0.01641  | -0.01343  | -0.01629  | -0.01323 | 24.5 | 32.6 | 42.9 | -1356 | 1801 | 2393 | 3157 | 7351
| alpaca-13b | -65.1 | -3620.0 | -86.5 | -102.9 | -66.6 | 23.6 | -0.0205  | -0.01541  | -0.01262  | -0.01366 | 24.5 | 31.2 | 44.3 | -1810 | 2238 | 2849 | 4048 | 9135
| mpt-7b-chat | -39.9 | -2106.0 | -38.5 | -93.2 | -58.8 | 25.0 | -0.01364  | -0.01249  | -0.01284  | -0.01032 | 23.5 | 35.1 | 41.4 | -1053 | 1377 | 2057 | 2430 | 5864
| RWKV-4-Raven-14B | -52.1 | -3080.0 | -83.9 | -110.5 | -71.2 | 23.8 | -0.01601  | -0.0274  | -0.00577  | -0.02157 | 22.0 | 34.7 | 43.3 | -1540 | 1594 | 2508 | 3134 | 7236
| oasst-pythia-12b | -75.3 | -4510.0 | -98.8 | -123.4 | -80.5 | 23.4 | -0.02352  | -0.03063  | -0.02149  | -0.02212 | 21.6 | 32.9 | 45.4 | -2255 | 2054 | 3126 | 4309 | 9489
| oasst-sft-1-pythia-12b | -130.5 | -30.0 | -24.4 | -203.9 | -4.7 | 22.3 | -0.001  | -0.00109  | -0.00046  | -0.0011 | 19.4 | 19.4 | 61.1 | -15 | 7 | 7 | 22 | 36
| gpt4all-13b-snoozy | -23.6 | -1158.0 | -37.4 | -97.1 | -58.3 | 24.5 | -0.01291  | -0.01323  | -0.00027  | -0.00713 | 18.3 | 44.8 | 37.0 | -579 | 565 | 1385 | 1144 | 3094
| fastchat-t5-3b | -93.1 | -3736.0 | -99.3 | -155.1 | -100.9 | 23.5 | -0.02403  | -0.03152  | -0.01309  | -0.01849 | 17.6 | 34.9 | 47.5 | -1868 | 1100 | 2184 | 2968 | 6252
| stablelm-tuned-alpha-7b | -127.8 | -3486.0 | -152.9 | -174.8 | -113.6 | 22.3 | -0.01882  | -0.02451  | -0.01604  | -0.01602 | 17.0 | 32.3 | 50.7 | -1743 | 881 | 1671 | 2624 | 5176
| dolly-v2-12b | -150.3 | -4712.0 | -171.3 | -206.8 | -135.6 | 22.1 | -0.03045  | -0.02367  | -0.01953  | -0.02246 | 15.3 | 29.6 | 55.1 | -2356 | 904 | 1751 | 3260 | 5915
| llama-13b | -167.8 | -4186.0 | -180.5 | -225.4 | -147.6 | 22.2 | -0.02691  | -0.02997  | -0.02065  | -0.01221 | 14.3 | 27.9 | 57.8 | -2093 | 691 | 1344 | 2784 | 4819

| Rating method | L2 distance with true winrates on test set |
|---|---|
| Sequential Elo | 232.6 |
| Bayeselo | 236.4 |
| Glicko | 296.0 |
| Glicko2 | 203.7 |
| Trueskill | 694.2 |
| Melo2 | 188.9 |
| Melo4 | 182.2 |
| Melo10 | 186.4 |
| Melo20 | 176.9 |